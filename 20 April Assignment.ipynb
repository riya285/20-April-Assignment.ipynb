{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b40837c-7627-436b-91ce-7af5ead3ce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "\n",
    "The k-nearest neighbors (KNN) algorithm is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until function evaluation. In KNN, an object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). It can be used for both classification and regression tasks.\n",
    "\n",
    "Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "Choosing the value of K in KNN is crucial and can impact the performance of the algorithm. A small value of K (e.g., 1 or 3) can be sensitive to noise, while a large value of K may smooth over patterns in the data. The choice of K depends on the specific dataset and problem. Common methods for choosing K include cross-validation, trying different values and observing the impact on performance, and using mathematical techniques to optimize K based on the characteristics of the data.\n",
    "\n",
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "KNN Classifier: Used for classification tasks where the output is a class label. The class is determined by a majority vote of its k nearest neighbors.\n",
    "\n",
    "KNN Regressor: Used for regression tasks where the output is a real-valued quantity. The predicted value for an instance is the average of the values of its k nearest neighbors.\n",
    "\n",
    "Q4. How do you measure the performance of KNN?\n",
    "\n",
    "Common performance metrics for KNN include accuracy, precision, recall, F1-score for classification tasks, and metrics like mean squared error or R-squared for regression tasks. Cross-validation is often used to get a more robust estimate of the model's performance.\n",
    "\n",
    "Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "The curse of dimensionality refers to the challenges and increased computational requirements that arise when working with high-dimensional data. In KNN, as the number of dimensions increases, the distance between data points tends to increase, and the concept of proximity becomes less meaningful. This can lead to a degradation in the performance of KNN in high-dimensional spaces.\n",
    "\n",
    "Q6. How do you handle missing values in KNN?\n",
    "\n",
    "KNN can be sensitive to missing values. Imputation techniques, such as filling missing values with the mean or median, can be applied before using KNN. Alternatively, a distance metric that handles missing values appropriately, or methods like k-NN imputation, can be used.\n",
    "\n",
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "\n",
    "KNN Classifier: Suitable for problems where the output is a discrete class label, such as image classification or spam detection.\n",
    "\n",
    "KNN Regressor: Suitable for problems where the output is a continuous value, such as predicting house prices or stock prices.\n",
    "\n",
    "The choice between classifier and regressor depends on the nature of the problem and the type of output variable.\n",
    "\n",
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "\n",
    "Strengths:\n",
    "\n",
    "Simple to understand and implement.\n",
    "No training phase; the model is instance-based.\n",
    "Weaknesses:\n",
    "\n",
    "Computationally expensive, especially with large datasets.\n",
    "Sensitive to irrelevant features and the choice of distance metric.\n",
    "Performance can degrade in high-dimensional spaces (curse of dimensionality).\n",
    "Addressing these weaknesses may involve dimensionality reduction techniques, feature scaling, careful feature selection, and optimizing the choice of hyperparameters such as K.\n",
    "\n",
    "Q9. Euclidean distance is the standard distance measure used in KNN. It is the straight-line distance between two points in Euclidean space. On the other hand, Manhattan distance is the sum of the absolute differences of their coordinates. In KNN, Manhattan distance can be more suitable for categorical or ordinal features, as it does not require a continuous space to calculate distances.\n",
    "\n",
    "    \n",
    "Q10. What is the role of feature scaling in KNN?\n",
    "\n",
    "Feature scaling is important in KNN because the algorithm is based on the concept of distance between data points. If features have different scales, those with larger scales may dominate the distance computations. Standardizing or normalizing features to a similar scale ensures that no single feature has undue influence on the distance calculations, improving the performance of KNN. Common techniques include z-score normalization and min-max scaling.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
